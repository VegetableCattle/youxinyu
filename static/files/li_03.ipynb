{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sys import path\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(dir_path, label, data):\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        f=open(dir_path+file_name,'r',encoding='utf8')\n",
    "        lines=f.readlines()\n",
    "        data.append([lines,label])\n",
    "    random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.Divide the dataset as train, development and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 20000\n",
      "Dev dataset size: 5000\n",
      "Test dataset size: 25000\n"
     ]
    }
   ],
   "source": [
    "# Split dataset to k folds\n",
    "def crossValidationSplit(data, k_folds):\n",
    "    data_split = list()\n",
    "    data_copy = list(data)\n",
    "    size = int(len(data) / k_folds)\n",
    "    for _ in range(k_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < size:\n",
    "            k = random.randrange(len(data_copy))\n",
    "            fold.append(data_copy.pop(k))\n",
    "        data_split.append(fold)\n",
    "    return data_split\n",
    "\n",
    "def splitDataToTrainAndDev(dataset, k_folds):\n",
    "    folds = crossValidationSplit(dataset, k_folds)\n",
    "    train_set, dev_set = [], []\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        dev_set = list()\n",
    "        for row in fold:\n",
    "                row_copy = list(row)\n",
    "                dev_set.append(row_copy)\n",
    "        break\n",
    "    return train_set, dev_set\n",
    "\n",
    "dataset_neg, dataset_pos = [], []\n",
    "readFile(path[0] + '/aclImdb/train/neg/', int(-1), dataset_neg)\n",
    "readFile(path[0] + '/aclImdb/train/pos/', int(1), dataset_pos)\n",
    "\n",
    "n_folds = 5\n",
    "train_neg, dev_neg = splitDataToTrainAndDev(dataset_neg, n_folds)\n",
    "train_pos, dev_pos = splitDataToTrainAndDev(dataset_pos, n_folds)\n",
    "\n",
    "\n",
    "test_neg, test_pos = [], []\n",
    "readFile(path[0] + '/aclImdb/test/neg/', int(-1), test_neg)\n",
    "readFile(path[0] + '/aclImdb/test/pos/', int(1), test_pos)\n",
    "\n",
    "\n",
    "train = train_neg + train_pos\n",
    "dev = dev_neg + dev_pos\n",
    "test = test_neg + test_pos\n",
    "\n",
    "print('Train dataset size: ' + str(len(train)))\n",
    "print('Dev dataset size: ' + str(len(dev)))\n",
    "print('Test dataset size: ' + str(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.Build a vocabulary as list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[‘the’ ‘I’ ‘happy’ … \\] \n",
    "\n",
    "You may omit rare words for example if the occurrence is less than five times \n",
    "\n",
    "A reverse index as the key value might be handy \n",
    "\n",
    "{“the”: 0, “I”:1, “happy”:2 , … }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'husband': [267, 360],\n",
       " 'is': [8861, 9081],\n",
       " 'woman': [800, 865],\n",
       " 'person': [565, 507],\n",
       " 'of': [9522, 9487],\n",
       " 'this': [9219, 8875],\n",
       " 'by': [4587, 4793],\n",
       " 'movie': [6575, 5639],\n",
       " 'working': [265, 334],\n",
       " 'responsible': [111, 100],\n",
       " '8': [168, 456],\n",
       " 'it': [8955, 8849],\n",
       " 'the': [9935, 9909],\n",
       " 'was': [6829, 6131],\n",
       " 'portrays': [57, 119],\n",
       " 'change': [285, 420],\n",
       " 'cheating': [45, 33],\n",
       " 'an': [4791, 4989],\n",
       " 's': [7193, 7262],\n",
       " 'any': [2687, 1879],\n",
       " 'clinton': [9, 9],\n",
       " 'that': [8298, 7901],\n",
       " 'because': [2720, 2267],\n",
       " 'years': [1182, 1739],\n",
       " 'written': [598, 571],\n",
       " 'a': [9688, 9662],\n",
       " 'to': [9473, 9290],\n",
       " 'who': [4339, 4725],\n",
       " 'era': [149, 268],\n",
       " 'wife': [543, 726],\n",
       " 'result': [267, 205],\n",
       " 'hard': [982, 919],\n",
       " 'wonder': [480, 280],\n",
       " 'little': [1878, 2017],\n",
       " 'as': [6172, 6707],\n",
       " 'bored': [305, 105],\n",
       " 'has': [3767, 4245],\n",
       " 'obvious': [485, 287],\n",
       " 'hired': [77, 62],\n",
       " 'much': [2957, 2699],\n",
       " 'nudity': [284, 117],\n",
       " '16': [52, 48],\n",
       " 'in': [8769, 8866],\n",
       " 'guy': [1090, 657],\n",
       " 'sight': [127, 117],\n",
       " 'bug': [31, 29],\n",
       " 'anyone': [985, 864],\n",
       " 'times': [991, 1257],\n",
       " 'al': [104, 114],\n",
       " 'ping': [10, 8],\n",
       " 'are': [5473, 5609],\n",
       " 'head': [591, 473],\n",
       " 'features': [212, 249],\n",
       " 'nice': [620, 763],\n",
       " 'part': [1256, 1307],\n",
       " 'likes': [174, 177],\n",
       " 'gets': [1077, 1034],\n",
       " 'slowly': [129, 189],\n",
       " 'spanish': [76, 91],\n",
       " 'girl': [848, 848],\n",
       " 'close': [474, 506],\n",
       " 'for': [7193, 7128],\n",
       " 'up': [3628, 3325],\n",
       " 'supporting': [220, 433],\n",
       " 'split': [53, 61],\n",
       " 'during': [695, 770],\n",
       " 'one': [5584, 5749],\n",
       " 'hilarious': [279, 443],\n",
       " 'pong': [9, 1],\n",
       " 'me': [3073, 2816],\n",
       " 'interview': [68, 66],\n",
       " 'million': [156, 118],\n",
       " 'cast': [1126, 1428],\n",
       " 'demand': [33, 36],\n",
       " '40': [124, 116],\n",
       " 'eyes': [394, 460],\n",
       " 'strong': [225, 557],\n",
       " 'listen': [120, 124],\n",
       " 'trying': [1050, 704],\n",
       " 'count': [147, 102],\n",
       " 'don': [3051, 2091],\n",
       " 'totally': [539, 368],\n",
       " 'black': [603, 612],\n",
       " 'arrived': [35, 37],\n",
       " 'also': [2116, 3084],\n",
       " 'or': [4542, 3706],\n",
       " 'dialog': [327, 187],\n",
       " 'tall': [46, 38],\n",
       " 'balls': [38, 23],\n",
       " 'highlight': [73, 88],\n",
       " 'fares': [6, 4],\n",
       " 'gore': [443, 181],\n",
       " 'really': [3210, 2890],\n",
       " 'should': [1841, 1412],\n",
       " 'nelson': [40, 48],\n",
       " 'didn': [1654, 1101],\n",
       " 'cannibals': [18, 8],\n",
       " 'sabrina': [5, 22],\n",
       " 'some': [4036, 3670],\n",
       " 'being': [2087, 1970],\n",
       " 'confirms': [11, 14],\n",
       " 'track': [149, 140],\n",
       " 'but': [7352, 7020],\n",
       " 'out': [4397, 4134],\n",
       " 'at': [5384, 4974],\n",
       " 'young': [803, 1376],\n",
       " 'island': [168, 127],\n",
       " 'contamination': [4, 2],\n",
       " 'world': [870, 1414],\n",
       " 'agonizing': [18, 2],\n",
       " 'minutes': [1361, 546],\n",
       " 'odd': [203, 204],\n",
       " 'section': [76, 78],\n",
       " 'subs': [4, 3],\n",
       " 'ransom': [18, 7],\n",
       " 'praying': [18, 10],\n",
       " '80': [226, 178],\n",
       " 'on': [6414, 6206],\n",
       " 'same': [1364, 1298],\n",
       " 'bulging': [10, 2],\n",
       " 'naked': [191, 95],\n",
       " 'god': [478, 257],\n",
       " 'offer': [163, 132],\n",
       " 'cliver': [5, 0],\n",
       " 'group': [405, 319],\n",
       " 'vietnam': [51, 49],\n",
       " 'film': [5476, 5668],\n",
       " 'scratching': [19, 9],\n",
       " 'more': [3517, 3738],\n",
       " 'what': [4157, 3762],\n",
       " 'whose': [277, 406],\n",
       " 'starlet': [8, 11],\n",
       " 'vet': [15, 14],\n",
       " 'when': [3483, 3741],\n",
       " 'going': [1499, 1194],\n",
       " 'no': [3985, 2621],\n",
       " 'mostly': [357, 336],\n",
       " 'cannibal': [38, 8],\n",
       " 'producer': [157, 156],\n",
       " 'laura': [39, 50],\n",
       " 'antonio': [19, 19],\n",
       " 'sadly': [270, 164],\n",
       " 'disc': [32, 51],\n",
       " 'variety': [55, 94],\n",
       " 'comes': [853, 896],\n",
       " 'monster': [224, 124],\n",
       " 'franco': [30, 17],\n",
       " 'jess': [17, 26],\n",
       " 'pads': [2, 3],\n",
       " 'darker': [15, 71],\n",
       " 'weston': [8, 1],\n",
       " 'things': [1134, 1204],\n",
       " 'astonishing': [13, 49],\n",
       " 'only': [3724, 2966],\n",
       " 'than': [2985, 2716],\n",
       " 'there': [4712, 3907],\n",
       " 'waves': [21, 36],\n",
       " 'value': [242, 162],\n",
       " 'were': [2972, 2443],\n",
       " 'most': [2376, 2787],\n",
       " 'fistfight': [4, 3],\n",
       " 'about': [4348, 4052],\n",
       " 'save': [541, 208],\n",
       " 'women': [566, 499],\n",
       " '1': [933, 353],\n",
       " 'scenes': [1644, 1455],\n",
       " 'ol': [22, 26],\n",
       " 'star': [625, 664],\n",
       " 'rugged': [2, 24],\n",
       " 'pretty': [1313, 992],\n",
       " 'splashing': [4, 2],\n",
       " 'his': [3941, 4576],\n",
       " 'they': [4714, 3972],\n",
       " 'death': [608, 620],\n",
       " 'into': [2564, 2579],\n",
       " 'full': [560, 678],\n",
       " 'ever': [2037, 1797],\n",
       " 'werner': [11, 10],\n",
       " 'tons': [51, 48],\n",
       " 'delivered': [89, 94],\n",
       " 'minute': [350, 220],\n",
       " 'bit': [885, 1149],\n",
       " 'better': [2147, 1544],\n",
       " 'seen': [2099, 2221],\n",
       " 'between': [943, 1242],\n",
       " 'hour': [564, 293],\n",
       " 'and': [9617, 9711],\n",
       " 'with': [6911, 7126],\n",
       " 'your': [1960, 1511],\n",
       " 'i': [8256, 7649],\n",
       " 'paint': [107, 35],\n",
       " 'de': [151, 230],\n",
       " 'wrestling': [45, 17],\n",
       " 'peter': [204, 271],\n",
       " 'him': [1871, 2233],\n",
       " 'long': [1207, 1176],\n",
       " 'thanks': [135, 234],\n",
       " 'you': [5645, 5294],\n",
       " 'entertainment': [301, 350],\n",
       " 'including': [323, 465],\n",
       " 'tribe': [28, 22],\n",
       " 'dvd': [674, 850],\n",
       " 'french': [153, 284],\n",
       " 'crawford': [20, 17],\n",
       " 'set': [824, 900],\n",
       " 'ursula': [15, 4],\n",
       " 'dub': [17, 25],\n",
       " 'looks': [1010, 595],\n",
       " 'option': [34, 29],\n",
       " 'not': [6295, 5607],\n",
       " 'local': [304, 318],\n",
       " 'goofy': [78, 56],\n",
       " '$6': [12, 2],\n",
       " 'shown': [320, 388],\n",
       " 'their': [2678, 2933],\n",
       " 'images': [117, 210],\n",
       " 'tops': [28, 21],\n",
       " 'filming': [169, 128],\n",
       " 'gives': [387, 724],\n",
       " 't': [6692, 5317],\n",
       " 'impossibly': [12, 8],\n",
       " 'be': [5945, 5347],\n",
       " 'run': [491, 393],\n",
       " 'say': [1872, 1545],\n",
       " 'kidnapped': [52, 49],\n",
       " 'pleasant': [66, 122],\n",
       " 'have': [5975, 5354],\n",
       " 'warrior': [42, 33],\n",
       " 'gojoe': [3, 6],\n",
       " 'budget': [787, 401],\n",
       " 'blurry': [17, 6],\n",
       " 'bridge': [38, 64],\n",
       " 'lies': [103, 111],\n",
       " 'fight': [359, 345],\n",
       " 'making': [1152, 860],\n",
       " 'production': [671, 524],\n",
       " 'filmmaker': [118, 105],\n",
       " 'spiritual': [37, 47],\n",
       " 'style': [478, 599],\n",
       " 'rival': [26, 86],\n",
       " 'conflict': [85, 112],\n",
       " 'worldly': [5, 16],\n",
       " 'flourishes': [6, 6],\n",
       " 'sogo': [1, 5],\n",
       " 'editing': [359, 208],\n",
       " 'teacher': [107, 92],\n",
       " 'night': [680, 728],\n",
       " 'clan': [15, 32],\n",
       " 'element': [119, 161],\n",
       " 'produced': [194, 205],\n",
       " 'find': [1240, 1477],\n",
       " 'under': [483, 531],\n",
       " 'realize': [247, 253],\n",
       " 'guards': [25, 14],\n",
       " 'help': [638, 705],\n",
       " 'could': [2780, 2022],\n",
       " 'which': [2998, 3068],\n",
       " 'its': [1881, 2282],\n",
       " 'montages': [16, 9],\n",
       " 'squarely': [8, 8],\n",
       " 'holy': [56, 26],\n",
       " 'however': [1179, 1177],\n",
       " 'tells': [232, 380],\n",
       " 'skilled': [20, 17],\n",
       " 'conditions': [25, 27],\n",
       " 'doesn': [1609, 1272],\n",
       " 'ishii': [2, 4],\n",
       " 'kind': [979, 847],\n",
       " 'every': [1333, 1298],\n",
       " 'enlightenment': [13, 7],\n",
       " 'destiny': [16, 39],\n",
       " 'main': [810, 720],\n",
       " 'he': [4021, 4277],\n",
       " 'right': [1049, 1153],\n",
       " 'earn': [24, 24],\n",
       " 'monk': [21, 34],\n",
       " 'absence': [44, 50],\n",
       " 'society': [147, 284],\n",
       " 'spirit': [130, 222],\n",
       " 'will': [2217, 2802],\n",
       " 'informs': [10, 22],\n",
       " 'disappointment': [239, 71],\n",
       " 'nearly': [310, 313],\n",
       " 'benkei': [3, 4],\n",
       " 'low': [820, 411],\n",
       " 'defeating': [11, 9],\n",
       " 'time': [3499, 3501],\n",
       " 'photography': [128, 184],\n",
       " 'bring': [310, 360],\n",
       " 'loose': [105, 100],\n",
       " 'decision': [88, 91],\n",
       " 'mysterious': [124, 166],\n",
       " 'compensate': [23, 9],\n",
       " 'hallmarks': [5, 3],\n",
       " 'might': [1128, 793],\n",
       " 'can': [3953, 3668],\n",
       " 'story': [2721, 3346],\n",
       " 'depiction': [44, 69],\n",
       " 'artsy': [33, 14],\n",
       " 'clumsy': [66, 19],\n",
       " 'hope': [490, 545],\n",
       " 'down': [1330, 1187],\n",
       " 'inferiority': [4, 1],\n",
       " 'never': [1984, 2056],\n",
       " 'herself': [222, 331],\n",
       " 'understand': [585, 558],\n",
       " 'ashamed': [102, 17],\n",
       " 'moral': [109, 148],\n",
       " 'player': [104, 102],\n",
       " 'needs': [289, 308],\n",
       " 'she': [2126, 2275],\n",
       " 'way': [2361, 2460],\n",
       " 'bad': [3477, 1191],\n",
       " 'dancing': [137, 181],\n",
       " 'beer': [55, 38],\n",
       " 'two': [1921, 2088],\n",
       " 'thing:': [28, 11],\n",
       " 'very': [3022, 3913],\n",
       " 'normal': [139, 196],\n",
       " 'bigoted': [10, 5],\n",
       " 'upper': [52, 66],\n",
       " 'understands': [19, 42],\n",
       " 'bunch': [424, 168],\n",
       " 'living': [303, 446],\n",
       " 'classes': [35, 41],\n",
       " 'just': [4725, 3695],\n",
       " 'minded': [73, 60],\n",
       " 'think': [2208, 2134],\n",
       " 'mother': [366, 497],\n",
       " 'so': [5088, 4326],\n",
       " 'good': [3879, 3827],\n",
       " 'tone': [141, 191],\n",
       " 'small': [471, 699],\n",
       " 'all': [5257, 5200],\n",
       " 'simply': [759, 561],\n",
       " 'our': [637, 845],\n",
       " 'having': [890, 881],\n",
       " 'load': [62, 32],\n",
       " 'realistic': [167, 353],\n",
       " 'terribly': [148, 58],\n",
       " 'yet': [829, 991],\n",
       " 'child': [351, 447],\n",
       " 'gaudy': [6, 7],\n",
       " 'associated': [56, 53],\n",
       " 'unwashed': [9, 2],\n",
       " 'piano': [31, 59],\n",
       " 'people': [2561, 2406],\n",
       " 'giving': [325, 310],\n",
       " 'sort': [583, 433],\n",
       " 'somehow': [321, 238],\n",
       " 'mind': [716, 713],\n",
       " 'spare': [49, 30],\n",
       " 'aspire': [5, 11],\n",
       " 'heights': [20, 45],\n",
       " 'incredibly': [275, 181],\n",
       " 'enjoying': [55, 78],\n",
       " 'we': [2349, 2391],\n",
       " 'been': [2928, 2407],\n",
       " 'neither': [266, 143],\n",
       " 'learns': [48, 130],\n",
       " 'daughter': [327, 358],\n",
       " 'stella': [19, 13],\n",
       " 'great': [1672, 3381],\n",
       " 'crude': [82, 65],\n",
       " 'see': [3114, 3378],\n",
       " 'snobby': [8, 7],\n",
       " 'disgusting': [122, 39],\n",
       " 'hogwash': [6, 3],\n",
       " 'made': [2721, 2390],\n",
       " 'class': [268, 328],\n",
       " 'manners': [9, 22],\n",
       " 'dress': [61, 73],\n",
       " 'cannot': [409, 355],\n",
       " 'her': [2400, 2725],\n",
       " 'snobs': [7, 6],\n",
       " 'keep': [559, 604],\n",
       " 'dialogue': [632, 416],\n",
       " 'performance': [625, 1181],\n",
       " 'fiercely': [5, 6],\n",
       " 'successful': [152, 231],\n",
       " 'rosemary': [13, 37],\n",
       " 'll': [1029, 884],\n",
       " 'order': [301, 355],\n",
       " 'meant': [254, 203],\n",
       " 'inspired': [87, 171],\n",
       " 'swain': [4, 3],\n",
       " 'work': [1349, 1457],\n",
       " 'brand': [54, 42],\n",
       " '10': [1321, 1302],\n",
       " 'laughing': [221, 190],\n",
       " 'moose': [3, 5],\n",
       " 'major': [343, 309],\n",
       " 'chase': [159, 130],\n",
       " 'charley': [3, 10],\n",
       " 'reminds': [84, 146],\n",
       " 'wooden': [204, 38],\n",
       " 'involving': [178, 174],\n",
       " 'watch': [2214, 2110],\n",
       " 'career': [330, 371],\n",
       " 'oliver': [51, 60],\n",
       " 'sometimes': [329, 500],\n",
       " '3': [822, 392],\n",
       " 'agree': [223, 222],\n",
       " 'believable': [189, 335],\n",
       " 'standard': [175, 153],\n",
       " 'had': [3238, 2771],\n",
       " 'screen': [802, 907],\n",
       " 'early': [467, 628],\n",
       " 'did': [2071, 1752],\n",
       " 'father': [438, 678],\n",
       " 'sunday': [45, 94],\n",
       " 'though': [1344, 1504],\n",
       " 'issue': [96, 102],\n",
       " 'charitable': [10, 4],\n",
       " 'hawk': [10, 10],\n",
       " 'chaplin': [30, 51],\n",
       " 'hand': [455, 409],\n",
       " 'rather': [917, 868],\n",
       " 'something': [1888, 1356],\n",
       " 'myers': [30, 21],\n",
       " 'rate': [215, 249],\n",
       " 'funny': [1305, 1188],\n",
       " 'foil': [19, 21],\n",
       " 'hardy': [36, 37],\n",
       " 'die': [324, 225],\n",
       " 'got': [1344, 1128],\n",
       " 'white': [404, 506],\n",
       " 'else': [855, 583],\n",
       " 'comics': [30, 47],\n",
       " 'role': [774, 1189],\n",
       " 'mentioned': [223, 229],\n",
       " 'page': [78, 101],\n",
       " 'flat': [329, 111],\n",
       " 'tend': [63, 107],\n",
       " 'k': [90, 75],\n",
       " 'placement': [22, 11],\n",
       " 'missing': [220, 198],\n",
       " 'john': [489, 764],\n",
       " 'films': [1608, 1998],\n",
       " 'wedding': [75, 109],\n",
       " 'faced': [70, 94],\n",
       " 'ingenue': [4, 2],\n",
       " 've': [1688, 1495],\n",
       " 'harry': [78, 116],\n",
       " 'wrong': [762, 493],\n",
       " 'naff': [4, 3],\n",
       " 'william': [132, 267],\n",
       " 'eager': [37, 32],\n",
       " 'loyal': [33, 51],\n",
       " 'm': [1787, 1247],\n",
       " 'here': [1809, 1651],\n",
       " 'positive': [227, 143],\n",
       " 'fan': [642, 690],\n",
       " 'credit': [198, 205],\n",
       " 'even': [3973, 2780],\n",
       " 'casting': [229, 223],\n",
       " 'give': [1259, 1107],\n",
       " 'fair': [187, 151],\n",
       " 'product': [105, 76],\n",
       " 'actress': [398, 425],\n",
       " 'awkward': [110, 81],\n",
       " 'married': [154, 266],\n",
       " 'flaw': [46, 59],\n",
       " 'isn': [1177, 864],\n",
       " 'post': [155, 189],\n",
       " 'whom': [192, 279],\n",
       " 'name': [630, 488],\n",
       " 'mighty': [34, 31],\n",
       " 'action': [890, 950],\n",
       " 'books': [146, 184],\n",
       " 'such': [1599, 1587],\n",
       " 'professionalism': [11, 5],\n",
       " 'court': [44, 83],\n",
       " 'found': [880, 901],\n",
       " 'cleese': [10, 6],\n",
       " 'historian': [8, 14],\n",
       " 'notable': [53, 75],\n",
       " 'hoping': [226, 89],\n",
       " 'scotland': [23, 37],\n",
       " 'admire': [40, 52],\n",
       " 'dealt': [45, 70],\n",
       " 'leo': [20, 35],\n",
       " 'chested': [3, 2],\n",
       " 'character': [1886, 1965],\n",
       " 'best': [1401, 2497],\n",
       " 'intrigued': [43, 47],\n",
       " 'gormless': [5, 1],\n",
       " 'everyone': [701, 856],\n",
       " 'like': [4971, 4327],\n",
       " 'physically': [50, 67],\n",
       " 'from': [4622, 4770],\n",
       " 'example': [511, 438],\n",
       " 'rose': [39, 71],\n",
       " 'kinda': [122, 84],\n",
       " 'first': [2474, 2685],\n",
       " 'happen': [413, 339],\n",
       " 'flick': [550, 326],\n",
       " 'music': [811, 1018],\n",
       " 'rest': [755, 582],\n",
       " 'search': [91, 132],\n",
       " 'look': [1485, 1308],\n",
       " 'rap': [31, 38],\n",
       " 'transition': [28, 42],\n",
       " 'age': [301, 490],\n",
       " 'matter': [410, 445],\n",
       " 'puts': [114, 169],\n",
       " 'warn': [85, 31],\n",
       " 'top': [571, 754],\n",
       " 'crime': [192, 293],\n",
       " 'seem': [807, 721],\n",
       " 'spell': [40, 57],\n",
       " 'guess': [665, 306],\n",
       " 'thing': [1839, 1129],\n",
       " 'los': [44, 45],\n",
       " 'cake': [42, 31],\n",
       " 'liners': [77, 85],\n",
       " 'layer': [8, 12],\n",
       " 'although': [745, 1020],\n",
       " 'especially': [708, 1134],\n",
       " 'plot': [2505, 1495],\n",
       " 'cutting': [97, 72],\n",
       " 'cgi': [148, 61],\n",
       " 'almost': [1056, 1053],\n",
       " 'worse': [827, 167],\n",
       " 'called': [571, 449],\n",
       " 'video': [660, 438],\n",
       " 'japan': [58, 114],\n",
       " 'journey': [63, 224],\n",
       " 'disappointed': [436, 276],\n",
       " 'come': [1137, 1106],\n",
       " 'viewed': [71, 86],\n",
       " 'well': [2579, 3475],\n",
       " 'unfortunately': [670, 319],\n",
       " 'witty': [63, 143],\n",
       " 'before': [1452, 1441],\n",
       " 'habits': [13, 9],\n",
       " 'decent': [577, 238],\n",
       " 'forgot': [83, 52],\n",
       " 'oh': [737, 269],\n",
       " 'butchered': [23, 9],\n",
       " 'then': [2553, 1958],\n",
       " 'thriller': [258, 318],\n",
       " 'dominican': [4, 1],\n",
       " 'republic': [20, 13],\n",
       " 'nothing': [1855, 873],\n",
       " 'thought': [1213, 1111],\n",
       " 'reasons': [200, 230],\n",
       " '70': [153, 153],\n",
       " 'goes': [888, 862],\n",
       " 'ridiculous': [570, 137],\n",
       " 'premise': [347, 167],\n",
       " 'treated': [106, 97],\n",
       " 'characters': [2035, 2016],\n",
       " 'them': [2324, 2196],\n",
       " 'turns': [429, 506],\n",
       " 'man': [1389, 1806],\n",
       " 'space': [237, 215],\n",
       " 'perhaps': [554, 622],\n",
       " 'turned': [399, 312],\n",
       " 'recent': [153, 227],\n",
       " 'treat': [67, 171],\n",
       " 'akira': [4, 13],\n",
       " 'hits': [97, 112],\n",
       " 'king': [198, 269],\n",
       " 'france': [55, 100],\n",
       " 'thailand': [6, 22],\n",
       " 'call': [423, 274],\n",
       " 'reversed': [15, 11],\n",
       " 'half': [898, 501],\n",
       " 'drain': [21, 13],\n",
       " 'few': [1421, 1323],\n",
       " 'revolves': [57, 61],\n",
       " 'expected': [296, 237],\n",
       " 'via': [59, 69],\n",
       " 'another': [1510, 1411],\n",
       " 'dating': [45, 38],\n",
       " 'comedy': [879, 1011],\n",
       " 'life': [1474, 2203],\n",
       " 'acting': [2618, 1633],\n",
       " 'know': [1998, 1804],\n",
       " 'waking': [21, 18],\n",
       " 'socio': [7, 7],\n",
       " 'guessing': [48, 62],\n",
       " 'start': [681, 539],\n",
       " 'fact': [1241, 1155],\n",
       " 'after': [2369, 2276],\n",
       " 'predictable': [469, 170],\n",
       " 'relies': [57, 29],\n",
       " 'edge': [107, 216],\n",
       " 'husbands': [20, 32],\n",
       " 'fighter': [44, 37],\n",
       " 'spoiler': [156, 99],\n",
       " 'how': [2678, 2390],\n",
       " 'men': [474, 636],\n",
       " 'england': [69, 148],\n",
       " 'does': [1764, 1895],\n",
       " 'situations': [169, 194],\n",
       " 'off': [2123, 1602],\n",
       " 'actually': [1593, 1169],\n",
       " 'effects': [878, 577],\n",
       " 'sound': [464, 400],\n",
       " 'dr': [182, 212],\n",
       " 'states': [106, 152],\n",
       " 'us': [990, 1204],\n",
       " 'alert': [38, 25],\n",
       " 'f': [198, 105],\n",
       " 'thinking': [528, 348],\n",
       " 'art': [334, 442],\n",
       " 'whole': [1227, 911],\n",
       " 'makes': [1234, 1582],\n",
       " 'match': [112, 138],\n",
       " '30': [279, 181],\n",
       " 'roles': [296, 519],\n",
       " 'dream': [165, 257],\n",
       " 'coming': [376, 426],\n",
       " 'around': [1278, 1095],\n",
       " 'game': [264, 291],\n",
       " 'taken': [379, 372],\n",
       " 'clever': [171, 222],\n",
       " 'animation': [172, 279],\n",
       " 'awakening': [13, 21],\n",
       " 'my': [3203, 3305],\n",
       " 'now': [1428, 1533],\n",
       " 'amelie': [4, 8],\n",
       " 'cheesy': [274, 153],\n",
       " 'x85and': [5, 5],\n",
       " 'ing': [20, 10],\n",
       " 'would': [3584, 2947],\n",
       " 'talking': [383, 308],\n",
       " 'subject': [235, 266],\n",
       " 'makers': [230, 121],\n",
       " '95': [30, 21],\n",
       " 'jewish': [46, 43],\n",
       " 'explain': [195, 139],\n",
       " 'apologize': [19, 11],\n",
       " 'seeing': [716, 813],\n",
       " 'kids': [528, 505],\n",
       " 'hey': [178, 123],\n",
       " 'upset': [67, 53],\n",
       " 'truly': [525, 696],\n",
       " 'donate': [7, 0],\n",
       " 'money': [1093, 476],\n",
       " 'theater': [332, 228],\n",
       " 'funds': [15, 9],\n",
       " 'keeps': [191, 287],\n",
       " 'screenwriter': [76, 40],\n",
       " 'boy': [430, 530],\n",
       " 'portrayed': [184, 255],\n",
       " 'came': [581, 677],\n",
       " 'went': [596, 486],\n",
       " 'mine': [87, 116],\n",
       " 'autistic': [13, 7],\n",
       " 'serious': [359, 358],\n",
       " 'other': [2588, 2786],\n",
       " 'portion': [38, 34],\n",
       " 'week': [148, 187],\n",
       " 'dropping': [40, 30],\n",
       " 'frankie': [15, 22],\n",
       " 'demanded': [13, 9],\n",
       " 'bringing': [66, 94],\n",
       " 'lives': [348, 621],\n",
       " 'suffering': [90, 98],\n",
       " 'disabled': [14, 24],\n",
       " 'joke': [328, 96],\n",
       " 'challenges': [16, 38],\n",
       " 'teenager': [74, 79],\n",
       " 'point': [1192, 879],\n",
       " 'encourage': [36, 27],\n",
       " 'breasts': [68, 30],\n",
       " 'mentally': [58, 59],\n",
       " 'families': [59, 121],\n",
       " 'back': [1564, 1629],\n",
       " 'parents': [200, 303],\n",
       " 'nanny': [11, 9],\n",
       " 'pool': [57, 57],\n",
       " 'program': [82, 72],\n",
       " 'festival': [100, 165],\n",
       " 'mad': [171, 159],\n",
       " 'producers': [244, 116],\n",
       " 'ers': [5, 3],\n",
       " 'want': [1436, 1060],\n",
       " 'pitched': [23, 13],\n",
       " 'suppose': [186, 115],\n",
       " 'old': [1382, 1402],\n",
       " 'may': [936, 1212],\n",
       " 'kattan': [11, 3],\n",
       " 'ground': [137, 128],\n",
       " 'okay': [332, 164],\n",
       " 'spectrum': [11, 17],\n",
       " 'attempted': [64, 44],\n",
       " 'chris': [109, 119],\n",
       " 'worst': [1621, 183],\n",
       " 'middle': [371, 343],\n",
       " 'stole': [40, 45],\n",
       " 'avoid': [473, 95],\n",
       " 'brilliant': [197, 609],\n",
       " 'snl': [42, 14],\n",
       " 'get': [2886, 2520],\n",
       " 'line': [662, 572],\n",
       " 'voice': [335, 374],\n",
       " 'high': [751, 718],\n",
       " 'miserably': [88, 11],\n",
       " 'movies': [2252, 1997],\n",
       " 'entire': [622, 414],\n",
       " 'category': [103, 62],\n",
       " 'falls': [348, 287],\n",
       " 'plague': [62, 33],\n",
       " 'former': [169, 212],\n",
       " 'c': [217, 180],\n",
       " 'dead': [665, 512],\n",
       " 'flailing': [9, 2],\n",
       " 'failed': [280, 96],\n",
       " 'series': [655, 893],\n",
       " 'escapades': [4, 6],\n",
       " 'wooing': [5, 3],\n",
       " 'running': [415, 298],\n",
       " 'son': [329, 447],\n",
       " 'less': [753, 651],\n",
       " 'taylor': [51, 100],\n",
       " 'over': [1988, 1857],\n",
       " 'somewhat': [313, 438],\n",
       " 'lacking': [143, 79],\n",
       " '60': [115, 102],\n",
       " 'locale': [21, 11],\n",
       " 'previous': [237, 232],\n",
       " 'fantastic': [105, 448],\n",
       " 'displaying': [17, 24],\n",
       " 'typecast': [9, 11],\n",
       " 'rescue': [90, 78],\n",
       " 'insomniac': [6, 10],\n",
       " 'while': [1481, 1785],\n",
       " 'skull': [32, 15],\n",
       " 'repeated': [80, 74],\n",
       " 'setting': [211, 274],\n",
       " 'others': [486, 687],\n",
       " 'playing': [529, 581],\n",
       " 'port': [13, 8],\n",
       " 'successfully': [57, 71],\n",
       " 'maria': [27, 37],\n",
       " 'adventure': [103, 222],\n",
       " 'blood': [418, 272],\n",
       " 'fiancee': [6, 3],\n",
       " 'fighting': [217, 200],\n",
       " 'lead': [502, 459],\n",
       " 'lean': [21, 21],\n",
       " 'year': [710, 855],\n",
       " 'moving': [192, 435],\n",
       " 'complete': [477, 301],\n",
       " 'reduces': [6, 3],\n",
       " 'briskly': [3, 5],\n",
       " 'egyptian': [16, 8],\n",
       " 'clutches': [3, 12],\n",
       " 'effect': [224, 242],\n",
       " 'hours': [408, 307],\n",
       " 'forces': [80, 119],\n",
       " 'twelve': [36, 36],\n",
       " 'lumpy': [8, 1],\n",
       " 'ancient': [81, 86],\n",
       " 'footage': [227, 172],\n",
       " 'cap': [20, 16],\n",
       " 'magician': [11, 18],\n",
       " 'becomes': [466, 507],\n",
       " 'idol': [17, 21],\n",
       " 'if': [4674, 3876],\n",
       " 'dashing': [17, 28],\n",
       " 'against': [464, 542],\n",
       " 'deal': [226, 312],\n",
       " 'upon': [301, 345],\n",
       " 'viewers': [253, 312],\n",
       " 'frank': [95, 209],\n",
       " 'representative': [9, 22],\n",
       " 'sitting': [205, 142],\n",
       " 'magic': [102, 206],\n",
       " 'those': [1430, 1596],\n",
       " 'length': [109, 147],\n",
       " 'entertainments': [2, 6],\n",
       " 'chandler': [10, 12],\n",
       " 'figure': [280, 280],\n",
       " 'events': [260, 369],\n",
       " 'unless': [414, 106],\n",
       " 'ongoing': [15, 17],\n",
       " 'pointed': [64, 34],\n",
       " 'skill': [58, 80],\n",
       " 'ray': [104, 137],\n",
       " 'director': [1504, 1349],\n",
       " 'mummified': [5, 2],\n",
       " 'gear': [26, 29],\n",
       " 'bela': [25, 22],\n",
       " 'achieving': [6, 20],\n",
       " 'alba': [12, 13],\n",
       " 'serial': [118, 114],\n",
       " 'lugosi': [30, 30],\n",
       " 'sect': [3, 4],\n",
       " 'original': [1041, 839],\n",
       " 'pushed': [47, 49],\n",
       " 'engages': [12, 6],\n",
       " 'princess': [39, 76],\n",
       " 'kong': [51, 85],\n",
       " 'open': [227, 253],\n",
       " 'large': [203, 217],\n",
       " 'vastly': [28, 23],\n",
       " 'silly': [434, 206],\n",
       " 'obviously': [518, 326],\n",
       " 'begins': [215, 334],\n",
       " 'mcqueen': [8, 17],\n",
       " 'lands': [30, 32],\n",
       " 'lot': [1245, 1375],\n",
       " 'leaving': [184, 188],\n",
       " 'rifle': [15, 20],\n",
       " 'thumbs': [39, 64],\n",
       " 'sliced': [10, 3],\n",
       " 'soon': [364, 493],\n",
       " 'sounded': [88, 33],\n",
       " 'senseless': [56, 21],\n",
       " 'teens': [71, 67],\n",
       " 'sequel': [312, 192],\n",
       " 'overpaid': [3, 5],\n",
       " 'swiss': [11, 13],\n",
       " 'bucks': [68, 33],\n",
       " 'salary': [8, 6],\n",
       " 'blasts': [7, 6],\n",
       " 'earth': [288, 246],\n",
       " 'weight': [53, 60],\n",
       " 'putting': [150, 146],\n",
       " 'many': [1809, 2273],\n",
       " 'sold': [55, 60],\n",
       " '20': [269, 214],\n",
       " 'approaching': [25, 17],\n",
       " 'ending': [784, 803],\n",
       " 'cheese': [80, 31],\n",
       " 'classic': [431, 788],\n",
       " '3k': [5, 0],\n",
       " 'consuming': [9, 6],\n",
       " 'cars': [89, 85],\n",
       " 'drivel': [91, 8],\n",
       " 'dog': [213, 186],\n",
       " 'holes': [181, 79],\n",
       " 'teen': [131, 87],\n",
       " 'shotgun': [30, 11],\n",
       " 'cool': [354, 320],\n",
       " 'looked': [467, 253],\n",
       " 'children': [396, 500],\n",
       " 'enjoyed': [266, 655],\n",
       " 'trite': [90, 29],\n",
       " 'pitifully': [6, 1],\n",
       " 'door': [157, 120],\n",
       " 'won': [547, 668],\n",
       " 'forget': [260, 279],\n",
       " 'doing': [686, 486],\n",
       " 'entertaining': [437, 617],\n",
       " 'terry': [22, 39],\n",
       " 'directed': [431, 480],\n",
       " 'object': [49, 50],\n",
       " 'house': [592, 558],\n",
       " 'maybe': [937, 651],\n",
       " 'west': [134, 159],\n",
       " 'take': [1235, 1165],\n",
       " 'probably': [992, 953],\n",
       " '1000': [15, 11],\n",
       " 'annoying': [541, 163],\n",
       " 'screaming': [141, 44],\n",
       " 'corpses': [38, 12],\n",
       " 'heavy': [176, 185],\n",
       " 'each': [635, 992],\n",
       " 'pliers': [4, 1],\n",
       " 'waters': [24, 43],\n",
       " 'slides': [8, 5],\n",
       " 'susan': [57, 51],\n",
       " 'genitalia': [7, 3],\n",
       " 'struck': [41, 62],\n",
       " 'elvis': [32, 18],\n",
       " 'glop': [4, 1],\n",
       " 'mildly': [103, 28],\n",
       " 'electric': [31, 29],\n",
       " 'repeatedly': [56, 42],\n",
       " 'knives': [18, 9],\n",
       " 'whatever': [327, 202],\n",
       " 'camera': [687, 481],\n",
       " 'front': [233, 213],\n",
       " 'cutlery': [4, 1],\n",
       " 'carving': [5, 4],\n",
       " 'shed': [25, 35],\n",
       " 'constant': [114, 105],\n",
       " 'forward': [288, 217],\n",
       " 'big': [1122, 1151],\n",
       " 'hell': [477, 258],\n",
       " 'looking': [968, 747],\n",
       " 'wildest': [5, 5],\n",
       " 'too': [2418, 2188],\n",
       " 'finally': [525, 580],\n",
       " 'gadget': [13, 3],\n",
       " 'dreams': [104, 177],\n",
       " 'pace': [165, 257],\n",
       " 'terrible': [886, 165],\n",
       " 'fast': [358, 288],\n",
       " 'extremely': [400, 352],\n",
       " 'huge': [374, 316],\n",
       " 'cartoon': [159, 138],\n",
       " 'situation': [198, 288],\n",
       " 'fawcett': [9, 16],\n",
       " 'pain': [129, 135],\n",
       " 'mis': [14, 6],\n",
       " 'catch': [135, 207],\n",
       " 'getting': [641, 532],\n",
       " 'still': [1500, 2055],\n",
       " 'patrick': [51, 77],\n",
       " 'alive': [164, 165],\n",
       " 'thrills': [48, 50],\n",
       " 'felt': [559, 487],\n",
       " 'played': [699, 1003],\n",
       " 'said': [827, 687],\n",
       " 'killed': [434, 299],\n",
       " 'course': [794, 969],\n",
       " 'grow': [59, 108],\n",
       " 'survive': [83, 107],\n",
       " 'ready': [128, 131],\n",
       " 'cheated': [51, 19],\n",
       " 'abusive': [29, 40],\n",
       " 'soup': [21, 24],\n",
       " 'says': [393, 356],\n",
       " 'escape': [154, 201],\n",
       " 'kitchen': [49, 38],\n",
       " 'basically': [438, 235],\n",
       " 'meets': [171, 326],\n",
       " 'drifted': [4, 3],\n",
       " 'changing': [71, 73],\n",
       " 'bed': [157, 120],\n",
       " 'summary': [75, 53],\n",
       " 'domestic': [23, 42],\n",
       " 'identity': [66, 125],\n",
       " 'iowa': [6, 12],\n",
       " 'punched': [11, 10],\n",
       " 'sleeping': [77, 65],\n",
       " 'level': [356, 326],\n",
       " 'assume': [117, 59],\n",
       " 'burning': [55, 56],\n",
       " 'powerful': [96, 332],\n",
       " 'believes': [72, 97],\n",
       " 'cans': [10, 4],\n",
       " 'abuse': [67, 72],\n",
       " 'face': [578, 551],\n",
       " 'julia': [37, 66],\n",
       " 'town': [304, 417],\n",
       " 'power': [258, 340],\n",
       " 'anal': [17, 6],\n",
       " 'suspicious': [37, 36],\n",
       " 'audience': [757, 711],\n",
       " 'thrill': [39, 40],\n",
       " 'wimpy': [14, 5],\n",
       " 'arranges': [7, 5],\n",
       " 'act': [485, 366],\n",
       " 'mention': [353, 256],\n",
       " 'live': [448, 636],\n",
       " 'faking': [10, 8],\n",
       " 'mouse': [35, 71],\n",
       " 're': [1537, 1155],\n",
       " 'sets': [283, 344],\n",
       " 'experience': [328, 474],\n",
       " 'cat': [153, 154],\n",
       " 'abused': [26, 28],\n",
       " 'roberts': [37, 45],\n",
       " 'need': [666, 607],\n",
       " 'provide': [106, 121],\n",
       " 'anything': [1286, 755],\n",
       " 'mr': [344, 440],\n",
       " 'enemy': [68, 77],\n",
       " 'stayed': [73, 69],\n",
       " 'real': [1379, 1603],\n",
       " 'ms': [107, 98],\n",
       " 'farrah': [9, 16],\n",
       " 'cheap': [475, 147],\n",
       " 'starts': [488, 386],\n",
       " 'care': [567, 391],\n",
       " '=': [35, 26],\n",
       " 'struggled': [11, 11],\n",
       " 'garbage': [294, 57],\n",
       " 'draw': [77, 75],\n",
       " 'recycled': [36, 19],\n",
       " 'talented': [201, 217],\n",
       " 'beyonce': [8, 1],\n",
       " 'hollywood': [537, 683],\n",
       " 'appreciate': [121, 259],\n",
       " 'groups': [35, 48],\n",
       " 'hypocritical': [9, 7],\n",
       " 'ago': [321, 436],\n",
       " 'borrowing': [14, 5],\n",
       " 'without': [1108, 1166],\n",
       " 'storyline': [340, 246],\n",
       " 'costumes': [144, 168],\n",
       " 'yeah': [243, 102],\n",
       " 'actual': [320, 253],\n",
       " 'involvement': [45, 47],\n",
       " 'voices': [74, 77],\n",
       " 'choreography': [29, 51],\n",
       " 'saw': [1037, 1166],\n",
       " 'industry': [132, 119],\n",
       " 'empire': [23, 45],\n",
       " 'throwback': [14, 10],\n",
       " 'sure': [1015, 875],\n",
       " 'scripts': [73, 38],\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SegmentLineToWords(string):\n",
    "    string=string.replace('<br />', '')\n",
    "    return set([x.lower() for x in re.split(r'[\\s|,|;|.|/|\\[|\\]|;|\\!|?|\\'|\\\\|\\)|\\(|\\\"|@|&|#|-|*|%|>|<|^|-]\\s*',string.strip()) if x])\n",
    "\n",
    "def buildVocabularyList(dataset):\n",
    "    dict_list = {} #{'word':[neg_count, pos_count]}\n",
    "    for row in dataset:\n",
    "        words = set() #Words that appear multiple times in the same comment are counted only once\n",
    "        words = words.union(SegmentLineToWords(str(row[0])))\n",
    "        for word in words:\n",
    "            if word not in dict_list:\n",
    "                dict_list[word] = [0,0]\n",
    "            if row[1] == -1:\n",
    "                dict_list[word][0] += 1\n",
    "            else:\n",
    "                dict_list[word][1] += 1\n",
    "    for word in list(dict_list.keys()):\n",
    "        if dict_list[word][0] + dict_list[word][1]<5:\n",
    "            del dict_list[word]\n",
    "    return dict_list\n",
    "train_dict = buildVocabularyList(train)\n",
    "train_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.Calculate the following probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of the occurrence\n",
    "\n",
    "P\\[“the”\\] = num of documents containing ‘the’ / num of all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[“the”] = 0.9922\n"
     ]
    }
   ],
   "source": [
    "def getProbabilityOfOccurrence(word):\n",
    "    if word not in train_dict:\n",
    "        return 0\n",
    "    else: \n",
    "        return (train_dict[word][0] + train_dict[word][1])/(len(train))\n",
    "print(\"P[“the”] = \" + str(getProbabilityOfOccurrence(\"the\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional probability based on the sentiment\n",
    "\n",
    "P\\[“the” | Positive\\]  = # of positive documents containing “the” / num of all positive review documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[“the” | Positive] = 0.9909\n",
      "P[“the” | negative] = 0.9935\n"
     ]
    }
   ],
   "source": [
    "def getPosConditionalProbability(word):\n",
    "    if word not in train_dict:\n",
    "        return 0\n",
    "    else:\n",
    "        return train_dict[word][1]/(len(train_pos))\n",
    "def getNegConditionalProbability(word):\n",
    "    if word not in train_dict:\n",
    "        return 0\n",
    "    else:\n",
    "        return train_dict[word][0]/(len(train_neg))\n",
    "\n",
    "print(\"P[“the” | Positive] = \" + str(getPosConditionalProbability(\"the\")))\n",
    "print(\"P[“the” | negative] = \" + str(getNegConditionalProbability(\"the\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d.Calculate accuracy using dev dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(review, smoothing_flag):\n",
    "    words = set()\n",
    "    words = words.union(SegmentLineToWords(review))\n",
    "    pos_probability = 1\n",
    "    neg_probability = 1\n",
    "    for word in words:\n",
    "        if smoothing_flag == 1:\n",
    "            pos_probability *= getPosConditionalProbabilityUsingSmoothing(word)\n",
    "            neg_probability *= getNegConditionalProbabilityUsingSmoothing(word)\n",
    "            \n",
    "        else:\n",
    "            #print(word)\n",
    "            #print(\"getPosConditionalProbability: \" + str(getPosConditionalProbability(word)))\n",
    "            #print(\"getNegConditionalProbability: \" + str(getNegConditionalProbability(word)))\n",
    "            pos_probability *= getPosConditionalProbability(word)\n",
    "            neg_probability *= getNegConditionalProbability(word)\n",
    "    #print(\"pos_probability: \" + str(pos_probability))\n",
    "    #print(\"neg_probability: \" + str(neg_probability))\n",
    "    \n",
    "    return 1 if pos_probability >= neg_probability else -1\n",
    "\n",
    "def accuracy_metric(test_dataset, smoothing_flag):\n",
    "    correct = 0\n",
    "    for row in test_dataset:\n",
    "        #print( predict(str(row[0]), smoothing_flag))\n",
    "        #print(row[1])\n",
    "        if row[1] == predict(str(row[0]), smoothing_flag):\n",
    "            correct += 1\n",
    "    return correct / float(len(test_dataset)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.180%\n"
     ]
    }
   ],
   "source": [
    "#train_dict\n",
    "print('Accuracy: %.3f%%' % accuracy_metric(dev, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct five fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [56.34, 56.64, 56.699999999999996, 56.58, 55.94]\n",
      "Mean Accuracy: 56.440%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_algorithm(pos_dataset, neg_dataset, k_folds, smoothing_flag):\n",
    "    pos_folds = crossValidationSplit(pos_dataset, k_folds)\n",
    "    neg_folds = crossValidationSplit(neg_dataset, k_folds)\n",
    "    scores = list()\n",
    "    for i in range(0,len(pos_folds)):\n",
    "        train_pos = list(pos_folds)\n",
    "        train_neg = list(neg_folds)\n",
    "        \n",
    "        train_pos.remove(pos_folds[i])\n",
    "        train_neg.remove(neg_folds[i])\n",
    "        \n",
    "        train_pos = sum(train_pos, [])\n",
    "        train_neg = sum(train_neg, [])\n",
    "        \n",
    "        dev_pos = list()\n",
    "        for row in pos_folds[i]:\n",
    "            row_copy = list(row)\n",
    "            dev_pos.append(row_copy)\n",
    "        dev_neg = list()\n",
    "        for row in neg_folds[i]:\n",
    "            row_copy = list(row)\n",
    "            dev_neg.append(row_copy)\n",
    "        \n",
    "        train = train_pos + train_neg\n",
    "        train_dict = buildVocabularyList(train)\n",
    "        dev = dev_pos + dev_neg\n",
    "        accuracy = accuracy_metric(dev, smoothing_flag)\n",
    "        scores.append(accuracy)\n",
    "    print('Scores: %s' % scores)\n",
    "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "smoothing_flag = 0\n",
    "evaluate_algorithm(dataset_pos, dataset_neg, 5, smoothing_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e.Do following experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the effect of Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_value = 1\n",
    "def getPosConditionalProbabilityUsingSmoothing(word):\n",
    "    if word not in train_dict:\n",
    "        return lambda_value/(2*lambda_value+len(train_pos))\n",
    "    else:\n",
    "        return (lambda_value + train_dict[word][1])/(2*lambda_value+len(train_pos))\n",
    "def getNegConditionalProbabilityUsingSmoothing(word):\n",
    "    if word not in train_dict:\n",
    "        return lambda_value/(2*lambda_value+len(train_neg))\n",
    "    else:\n",
    "        return (lambda_value+train_dict[word][0])/(2*lambda_value+len(train_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy not using Smoothing: 55.180%\n",
      "Accuracy by using Smoothing: 80.740%\n"
     ]
    }
   ],
   "source": [
    "smoothing_flag = 1\n",
    "\n",
    "print('Accuracy not using Smoothing: %.3f%%' % accuracy_metric(dev, 0))\n",
    "print('Accuracy by using Smoothing: %.3f%%' % accuracy_metric(dev, smoothing_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive Top 10 words that predicts positive and negative class\n",
    "### P\\[Positive| word\\] \n",
    "P\\[Positive| word\\] = P\\[word| Positive\\] * P\\[Positive\\] / P\\[word\\]\n",
    "\n",
    "P\\[word\\] = P\\[word& Positive\\] + P\\[word& negative\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive Top 10 words that predicts positive and negative class by using train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words that predicts positive by using train data :\n",
      "edie\n",
      "philo\n",
      "goldsworthy\n",
      "antwone\n",
      "visconti\n",
      "gackt\n",
      "tsui\n",
      "mcintire\n",
      "din\n",
      "gunga\n",
      "\n",
      "Top 10 words that predicts negative by using train data :\n",
      "semblance\n",
      "ajay\n",
      "boll\n",
      "savini\n",
      "uwe\n",
      "dreck\n",
      "slater\n",
      "devgan\n",
      "btk\n",
      "stinker\n"
     ]
    }
   ],
   "source": [
    "def getTop10UsingTrain(label):\n",
    "    positive_list = []\n",
    "    for word in list(train_dict.keys()):\n",
    "        #value = ((train_dict[word][1] / len(train_pos)) * (len(train_pos) / (len(train)))) / (train_dict[word][1]/len(train_pos)*(len(train_pos) / len(train)) + train_dict[word][0])/len(train_neg)*(len(train_neg) / len(train))\n",
    "        value = (train_dict[word][label] + lambda_value) / (train_dict[word][1] + lambda_value + train_dict[word][0] + lambda_value)\n",
    "        positive_list.append([word,value])\n",
    "    return positive_list\n",
    "positive_list = np.array(getTop10UsingTrain(1))\n",
    "positive_list = positive_list[np.lexsort(positive_list.T)]\n",
    "\n",
    "negative_list = np.array(getTop10UsingTrain(0))\n",
    "negative_list = negative_list[np.lexsort(negative_list.T)]\n",
    "def printTop10(data_list):\n",
    "    for i in range(1, len(data_list)):\n",
    "        if i <= 10:\n",
    "            print(data_list[-i][0])\n",
    "        else:\n",
    "            break\n",
    "print(\"Top 10 words that predicts positive by using train data :\")\n",
    "printTop10(positive_list)\n",
    "print(\"\")\n",
    "print(\"Top 10 words that predicts negative by using train data :\")\n",
    "printTop10(negative_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see,if only the training set and formula are used to count the data obtained, the performance is not good. So the in the follows i will use the dev and algorithm to get top 10 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDevPredictsList():\n",
    "    predicts_list = {}\n",
    "    correct = 0\n",
    "    for row in dev:\n",
    "        if row[1] == predict(str(row[0]), 1):\n",
    "            words = set()\n",
    "            words = words.union(SegmentLineToWords(str(row[0])))\n",
    "            for word in words:\n",
    "                if word not in predicts_list:\n",
    "                    predicts_list[word] = [0,0]\n",
    "                if row[1] == -1:\n",
    "                    predicts_list[word][0] += 1\n",
    "                else:\n",
    "                    predicts_list[word][1] += 1\n",
    "    return predicts_list\n",
    "predicts_list = getDevPredictsList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words that predicts positive by using dev data :\n",
      "freedom\n",
      "sidney\n",
      "marvelous\n",
      "favorites\n",
      "delight\n",
      "hoffman\n",
      "perfection\n",
      "curtis\n",
      "survival\n",
      "flawless\n",
      "\n",
      "Top 10 words that predicts negative by using dev data :\n",
      "unfunny\n",
      "mst3k\n",
      "waste\n",
      "pointless\n",
      "tripe\n",
      "uninspiring\n",
      "hackneyed\n",
      "boll\n",
      "uwe\n",
      "ugh\n"
     ]
    }
   ],
   "source": [
    "def getTop10UsingDev(label):\n",
    "    positive_list = []\n",
    "    for word in list(predicts_list.keys()):\n",
    "        #value = ((train_dict[word][1] / len(train_pos)) * (len(train_pos) / (len(train)))) / (train_dict[word][1]/len(train_pos)*(len(train_pos) / len(train)) + train_dict[word][0])/len(train_neg)*(len(train_neg) / len(train))\n",
    "        #value = float(train_dict[word][1]) / float(len(train) * (train_dict[word][1] + train_dict[word][0]))\n",
    "        value = (predicts_list[word][label] + lambda_value) / (predicts_list[word][1] + lambda_value + predicts_list[word][0] + lambda_value)\n",
    "        positive_list.append([word,value])\n",
    "    return positive_list\n",
    "positive_list = np.array(getTop10UsingDev(1))\n",
    "positive_list = positive_list[np.lexsort(positive_list.T)]\n",
    "\n",
    "negative_list = np.array(getTop10UsingDev(0))\n",
    "negative_list = negative_list[np.lexsort(negative_list.T)]\n",
    "\n",
    "print(\"Top 10 words that predicts positive by using dev data :\")\n",
    "printTop10(positive_list)\n",
    "print(\"\")\n",
    "print(\"Top 10 words that predicts negative by using dev data :\")\n",
    "printTop10(negative_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f.Using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by using Smoothing of test dataset: 79.552%\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy by using Smoothing of test dataset: %.3f%%' % accuracy_metric(test, smoothing_flag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
